code_snippet,type,score
"	public static final void close(final Closeable pCloseable) {
		if(pCloseable != null) {
			try {
				pCloseable.close();
			} catch (final IOException e) {
				e.printStackTrace();
			}
		}
	}",method,
"		if(pCloseable != null) {
			try {
				pCloseable.close();
			} catch (final IOException e) {
				e.printStackTrace();
			}
		}",method,
"	public static final void flushCloseStream(final OutputStream pOutputStream) {
		if(pOutputStream != null) {
			try {
				pOutputStream.flush();
			} catch (final IOException e) {
				e.printStackTrace();
			} finally {
				StreamUtils.close(pOutputStream);
			}
		}
	}",method,
"		if(pOutputStream != null) {
			try {
				pOutputStream.flush();
			} catch (final IOException e) {
				e.printStackTrace();
			} finally {
				StreamUtils.close(pOutputStream);
			}
		}",method,
"	public static final void flushCloseWriter(final Writer pWriter) {
		if(pWriter != null) {
			try {
				pWriter.flush();
			} catch (final IOException e) {
				e.printStackTrace();
			} finally {
				StreamUtils.close(pWriter);
			}
		}
	}",method,
"		if(pWriter != null) {
			try {
				pWriter.flush();
			} catch (final IOException e) {
				e.printStackTrace();
			} finally {
				StreamUtils.close(pWriter);
			}
		}",method,
"public class RSAPrivateKeySpec implements KeySpec {
    private BigInteger modulus;
    private BigInteger privateExponent;
    public RSAPrivateKeySpec(BigInteger modulus, BigInteger privateExponent) {
        this.modulus = modulus;
        this.privateExponent = privateExponent;
    }
    public BigInteger getModulus() {
        return this.modulus;
    }
    public BigInteger getPrivateExponent() {
        return this.privateExponent;
    }
}",class,
"    public RSAPrivateKeySpec(BigInteger modulus, BigInteger privateExponent) {
        this.modulus = modulus;
        this.privateExponent = privateExponent;
    }",method,
"    public BigInteger getModulus() {
        return this.modulus;
    }",method,
"    public BigInteger getPrivateExponent() {
        return this.privateExponent;
    }",method,
"import org.apache.logging.log4j.spi.ExtendedLoggerWrapper;
class Log4J2Logger extends ExtendedLoggerWrapper implements InternalLogger {
    private static final long serialVersionUID = 5485418394879791397L;
    private static final String EXCEPTION_MESSAGE = ""Unexpected exception:"";
    Log4J2Logger(Logger logger) {
        super((ExtendedLogger) logger, logger.getName(), logger.getMessageFactory());
    }
    @Override
    public String name() {
        return getName();
    }
    @Override
    public void trace(Throwable t) {
        log(Level.TRACE, EXCEPTION_MESSAGE, t);
    }
    @Override
    public void debug(Throwable t) {
        log(Level.DEBUG, EXCEPTION_MESSAGE, t);
    }
    @Override
    public void info(Throwable t) {
        log(Level.INFO, EXCEPTION_MESSAGE, t);
    }
    @Override
    public void warn(Throwable t) {
        log(Level.WARN, EXCEPTION_MESSAGE, t);
    }
    @Override
    public void error(Throwable t) {
        log(Level.ERROR, EXCEPTION_MESSAGE, t);
    }
    @Override
    public boolean isEnabled(InternalLogLevel level) {
        return isEnabled(toLevel(level));
    }
    @Override
    public void log(InternalLogLevel level, String msg) {
        log(toLevel(level), msg);
    }
    @Override
    public void log(InternalLogLevel level, String format, Object arg) {
        log(toLevel(level), format, arg);
    }
    @Override
    public void log(InternalLogLevel level, String format, Object argA, Object argB) {
        log(toLevel(level), format, argA, argB);
    }
    @Override
    public void log(InternalLogLevel level, String format, Object... arguments) {
        log(toLevel(level), format, arguments);
    }
    @Override
    public void log(InternalLogLevel level, String msg, Throwable t) {
        log(toLevel(level), msg, t);
    }
    @Override
    public void log(InternalLogLevel level, Throwable t) {
        log(toLevel(level), EXCEPTION_MESSAGE, t);
    }
    protected Level toLevel(InternalLogLevel level) {
        switch (level) {
            case INFO:
                return Level.INFO;
            case DEBUG:
                return Level.DEBUG;
            case WARN:
                return Level.WARN;
            case ERROR:
                return Level.ERROR;
            case TRACE:
                return Level.TRACE;
            default:
                throw new Error();
        }
    }
}",class,
"    Log4J2Logger(Logger logger) {
        super((ExtendedLogger) logger, logger.getName(), logger.getMessageFactory());
    }",method,
"    @Override
    public String name() {
        return getName();
    }",method,
"    @Override
    public void trace(Throwable t) {
        log(Level.TRACE, EXCEPTION_MESSAGE, t);
    }",method,
"    @Override
    public void debug(Throwable t) {
        log(Level.DEBUG, EXCEPTION_MESSAGE, t);
    }",method,
"    @Override
    public void info(Throwable t) {
        log(Level.INFO, EXCEPTION_MESSAGE, t);
    }",method,
"    @Override
    public void warn(Throwable t) {
        log(Level.WARN, EXCEPTION_MESSAGE, t);
    }",method,
"    @Override
    public void error(Throwable t) {
        log(Level.ERROR, EXCEPTION_MESSAGE, t);
    }",method,
"    @Override
    public boolean isEnabled(InternalLogLevel level) {
        return isEnabled(toLevel(level));
    }",method,
"    @Override
    public void log(InternalLogLevel level, String msg) {
        log(toLevel(level), msg);
    }",method,
"    @Override
    public void log(InternalLogLevel level, String format, Object arg) {
        log(toLevel(level), format, arg);
    }",method,
"    @Override
    public void log(InternalLogLevel level, String format, Object argA, Object argB) {
        log(toLevel(level), format, argA, argB);
    }",method,
"    @Override
    public void log(InternalLogLevel level, String format, Object... arguments) {
        log(toLevel(level), format, arguments);
    }",method,
"    @Override
    public void log(InternalLogLevel level, String msg, Throwable t) {
        log(toLevel(level), msg, t);
    }",method,
"    @Override
    public void log(InternalLogLevel level, Throwable t) {
        log(toLevel(level), EXCEPTION_MESSAGE, t);
    }",method,
"    protected Level toLevel(InternalLogLevel level) {
        switch (level) {
            case INFO:
                return Level.INFO;
            case DEBUG:
                return Level.DEBUG;
            case WARN:
                return Level.WARN;
            case ERROR:
                return Level.ERROR;
            case TRACE:
                return Level.TRACE;
            default:
                throw new Error();
        }
    }",method,
"        switch (level) {
            case INFO:
                return Level.INFO;
            case DEBUG:
                return Level.DEBUG;
            case WARN:
                return Level.WARN;
            case ERROR:
                return Level.ERROR;
            case TRACE:
                return Level.TRACE;
            default:
                throw new Error();
        }",method,
"public class FastJsonRedisSerializerTest {
    private FastJsonRedisSerializer<User> serializer;
    @Before
    public void setUp() {
        this.serializer = new FastJsonRedisSerializer<User>(User.class);
    }
    @Test
    public void test_1() {
        User user = serializer.deserialize(serializer.serialize(new User(1, ""土豆"", 25)));
        Assert.assertTrue(Objects.equal(user.getId(), 1));
        Assert.assertTrue(Objects.equal(user.getName(), ""土豆""));
        Assert.assertTrue(Objects.equal(user.getAge(), 25));
    }
    @Test
    public void test_2() {
        Assert.assertThat(serializer.serialize(null), Is.is(new byte[0]));
    }
    @Test
    public void test_3() {
        Assert.assertThat(serializer.deserialize(new byte[0]), IsNull.nullValue());
    }
    @Test
    public void test_4() {
        Assert.assertThat(serializer.deserialize(null), IsNull.nullValue());
    }
    @Test
    public void test_5() {
        User user = new User(1, ""土豆"", 25);
        byte[] serializedValue = serializer.serialize(user);
        Arrays.sort(serializedValue); // corrupt serialization result
        Assert.assertNull(serializer.deserialize(serializedValue));
    }
    static class User {
        private Integer id;
        private String name;
        private Integer age;
        public User() {
        }
        public User(Integer id, String name, Integer age) {
            this.id = id;
            this.name = name;
            this.age = age;
        }
        public Integer getId() {
            return id;
        }
        public void setId(Integer id) {
            this.id = id;
        }
        public String getName() {
            return name;
        }
        public void setName(String name) {
            this.name = name;
        }
        public Integer getAge() {
            return age;
        }
        public void setAge(Integer age) {
            this.age = age;
        }
    }
}",class,
"    static class User {
        private Integer id;
        private String name;
        private Integer age;
        public User() {
        }
        public User(Integer id, String name, Integer age) {
            this.id = id;
            this.name = name;
            this.age = age;
        }
        public Integer getId() {
            return id;
        }
        public void setId(Integer id) {
            this.id = id;
        }
        public String getName() {
            return name;
        }
        public void setName(String name) {
            this.name = name;
        }
        public Integer getAge() {
            return age;
        }
        public void setAge(Integer age) {
            this.age = age;
        }
    }",class,
"    @Before
    public void setUp() {
        this.serializer = new FastJsonRedisSerializer<User>(User.class);
    }",method,
"    @Test
    public void test_1() {
        User user = serializer.deserialize(serializer.serialize(new User(1, ""土豆"", 25)));
        Assert.assertTrue(Objects.equal(user.getId(), 1));
        Assert.assertTrue(Objects.equal(user.getName(), ""土豆""));
        Assert.assertTrue(Objects.equal(user.getAge(), 25));
    }",method,
"    @Test
    public void test_2() {
        Assert.assertThat(serializer.serialize(null), Is.is(new byte[0]));
    }",method,
"    @Test
    public void test_3() {
        Assert.assertThat(serializer.deserialize(new byte[0]), IsNull.nullValue());
    }",method,
"    @Test
    public void test_4() {
        Assert.assertThat(serializer.deserialize(null), IsNull.nullValue());
    }",method,
"    @Test
    public void test_5() {
        User user = new User(1, ""土豆"", 25);
        byte[] serializedValue = serializer.serialize(user);
        Arrays.sort(serializedValue); // corrupt serialization result
        Assert.assertNull(serializer.deserialize(serializedValue));
    }",method,
"        public User() {
        }",method,
"        public User(Integer id, String name, Integer age) {
            this.id = id;
            this.name = name;
            this.age = age;
        }",method,
"        public Integer getId() {
            return id;
        }",method,
"        public void setId(Integer id) {
            this.id = id;
        }",method,
"        public String getName() {
            return name;
        }",method,
"        public void setName(String name) {
            this.name = name;
        }",method,
"        public Integer getAge() {
            return age;
        }",method,
"        public void setAge(Integer age) {
            this.age = age;
        }",method,
"public class BitSetFilterCacheTests extends ESTestCase {
    private static final IndexSettings INDEX_SETTINGS = IndexSettingsModule.newIndexSettings(""test"", Settings.EMPTY);
    private static int matchCount(BitSetProducer producer, IndexReader reader) throws IOException {
        int count = 0;
        for (LeafReaderContext ctx : reader.leaves()) {
            final BitSet bitSet = producer.getBitSet(ctx);
            if (bitSet != null) {
                count += bitSet.cardinality();
            }
        }
        return count;
    }
    public void testInvalidateEntries() throws Exception {
        IndexWriter writer = new IndexWriter(
                new RAMDirectory(),
                new IndexWriterConfig(new StandardAnalyzer()).setMergePolicy(new LogByteSizeMergePolicy())
        );
        Document document = new Document();
        document.add(new StringField(""field"", ""value"", Field.Store.NO));
        writer.addDocument(document);
        writer.commit();
        document = new Document();
        document.add(new StringField(""field"", ""value"", Field.Store.NO));
        writer.addDocument(document);
        writer.commit();
        document = new Document();
        document.add(new StringField(""field"", ""value"", Field.Store.NO));
        writer.addDocument(document);
        writer.commit();
        DirectoryReader reader = DirectoryReader.open(writer);
        reader = ElasticsearchDirectoryReader.wrap(reader, new ShardId(""test"", ""_na_"", 0));
        IndexSearcher searcher = new IndexSearcher(reader);
        BitsetFilterCache cache = new BitsetFilterCache(INDEX_SETTINGS, new BitsetFilterCache.Listener() {
            @Override
            public void onCache(ShardId shardId, Accountable accountable) {
            }
            @Override
            public void onRemoval(ShardId shardId, Accountable accountable) {
            }
        });
        BitSetProducer filter = cache.getBitSetProducer(new TermQuery(new Term(""field"", ""value"")));
        assertThat(matchCount(filter, reader), equalTo(3));
        // now cached
        assertThat(matchCount(filter, reader), equalTo(3));
        // There are 3 segments
        assertThat(cache.getLoadedFilters().weight(), equalTo(3L));
        writer.forceMerge(1);
        reader.close();
        reader = DirectoryReader.open(writer);
        reader = ElasticsearchDirectoryReader.wrap(reader, new ShardId(""test"", ""_na_"", 0));
        searcher = new IndexSearcher(reader);
        assertThat(matchCount(filter, reader), equalTo(3));
        // now cached
        assertThat(matchCount(filter, reader), equalTo(3));
        // Only one segment now, so the size must be 1
        assertThat(cache.getLoadedFilters().weight(), equalTo(1L));
        reader.close();
        writer.close();
        // There is no reference from readers and writer to any segment in the test index, so the size in the fbs cache must be 0
        assertThat(cache.getLoadedFilters().weight(), equalTo(0L));
    }
    public void testListener() throws IOException {
        IndexWriter writer = new IndexWriter(
                new RAMDirectory(),
                new IndexWriterConfig(new StandardAnalyzer()).setMergePolicy(new LogByteSizeMergePolicy())
        );
        Document document = new Document();
        document.add(new StringField(""field"", ""value"", Field.Store.NO));
        writer.addDocument(document);
        writer.commit();
        final DirectoryReader writerReader = DirectoryReader.open(writer);
        final IndexReader reader = ElasticsearchDirectoryReader.wrap(writerReader, new ShardId(""test"", ""_na_"", 0));
        final AtomicLong stats = new AtomicLong();
        final AtomicInteger onCacheCalls = new AtomicInteger();
        final AtomicInteger onRemoveCalls = new AtomicInteger();
        final BitsetFilterCache cache = new BitsetFilterCache(INDEX_SETTINGS, new BitsetFilterCache.Listener() {
            @Override
            public void onCache(ShardId shardId, Accountable accountable) {
                onCacheCalls.incrementAndGet();
                stats.addAndGet(accountable.ramBytesUsed());
                if (writerReader != reader) {
                    assertNotNull(shardId);
                    assertEquals(""test"", shardId.getIndexName());
                    assertEquals(0, shardId.id());
                } else {
                    assertNull(shardId);
                }
            }
            @Override
            public void onRemoval(ShardId shardId, Accountable accountable) {
                onRemoveCalls.incrementAndGet();
                stats.addAndGet(-accountable.ramBytesUsed());
                if (writerReader != reader) {
                    assertNotNull(shardId);
                    assertEquals(""test"", shardId.getIndexName());
                    assertEquals(0, shardId.id());
                } else {
                    assertNull(shardId);
                }
            }
        });
        BitSetProducer filter = cache.getBitSetProducer(new TermQuery(new Term(""field"", ""value"")));
        assertThat(matchCount(filter, reader), equalTo(1));
        assertTrue(stats.get() > 0);
        assertEquals(1, onCacheCalls.get());
        assertEquals(0, onRemoveCalls.get());
        IOUtils.close(reader, writer);
        assertEquals(1, onRemoveCalls.get());
        assertEquals(0, stats.get());
    }
    public void testSetNullListener() {
        try {
            new BitsetFilterCache(INDEX_SETTINGS, null);
            fail(""listener can't be null"");
        } catch (IllegalArgumentException ex) {
            assertEquals(""listener must not be null"", ex.getMessage());
            // all is well
        }
    }
    public void testRejectOtherIndex() throws IOException {
        BitsetFilterCache cache = new BitsetFilterCache(INDEX_SETTINGS, new BitsetFilterCache.Listener() {
            @Override
            public void onCache(ShardId shardId, Accountable accountable) {
            }
            @Override
            public void onRemoval(ShardId shardId, Accountable accountable) {
            }
        });
        Directory dir = newDirectory();
        IndexWriter writer = new IndexWriter(
                dir,
                newIndexWriterConfig()
        );
        writer.addDocument(new Document());
        DirectoryReader reader = DirectoryReader.open(writer);
        writer.close();
        reader = ElasticsearchDirectoryReader.wrap(reader, new ShardId(""test2"", ""_na_"", 0));
        BitSetProducer producer = cache.getBitSetProducer(new MatchAllDocsQuery());
        try {
            producer.getBitSet(reader.leaves().get(0));
            fail();
        } catch (IllegalStateException expected) {
            assertEquals(""Trying to load bit set for index [test2] with cache of index [test]"", expected.getMessage());
        } finally {
            IOUtils.close(reader, dir);
        }
    }
}",class,
"    private static int matchCount(BitSetProducer producer, IndexReader reader) throws IOException {
        int count = 0;
        for (LeafReaderContext ctx : reader.leaves()) {
            final BitSet bitSet = producer.getBitSet(ctx);
            if (bitSet != null) {
                count += bitSet.cardinality();
            }
        }
        return count;
    }",method,
"            if (bitSet != null) {
                count += bitSet.cardinality();
            }",method,
"    public void testInvalidateEntries() throws Exception {
        IndexWriter writer = new IndexWriter(
                new RAMDirectory(),
                new IndexWriterConfig(new StandardAnalyzer()).setMergePolicy(new LogByteSizeMergePolicy())
        );
        Document document = new Document();
        document.add(new StringField(""field"", ""value"", Field.Store.NO));
        writer.addDocument(document);
        writer.commit();
        document = new Document();
        document.add(new StringField(""field"", ""value"", Field.Store.NO));
        writer.addDocument(document);
        writer.commit();
        document = new Document();
        document.add(new StringField(""field"", ""value"", Field.Store.NO));
        writer.addDocument(document);
        writer.commit();
        DirectoryReader reader = DirectoryReader.open(writer);
        reader = ElasticsearchDirectoryReader.wrap(reader, new ShardId(""test"", ""_na_"", 0));
        IndexSearcher searcher = new IndexSearcher(reader);
        BitsetFilterCache cache = new BitsetFilterCache(INDEX_SETTINGS, new BitsetFilterCache.Listener() {
            @Override
            public void onCache(ShardId shardId, Accountable accountable) {
            }
            @Override
            public void onRemoval(ShardId shardId, Accountable accountable) {
            }
        });
        BitSetProducer filter = cache.getBitSetProducer(new TermQuery(new Term(""field"", ""value"")));
        assertThat(matchCount(filter, reader), equalTo(3));
        // now cached
        assertThat(matchCount(filter, reader), equalTo(3));
        // There are 3 segments
        assertThat(cache.getLoadedFilters().weight(), equalTo(3L));
        writer.forceMerge(1);
        reader.close();
        reader = DirectoryReader.open(writer);
        reader = ElasticsearchDirectoryReader.wrap(reader, new ShardId(""test"", ""_na_"", 0));
        searcher = new IndexSearcher(reader);
        assertThat(matchCount(filter, reader), equalTo(3));
        // now cached
        assertThat(matchCount(filter, reader), equalTo(3));
        // Only one segment now, so the size must be 1
        assertThat(cache.getLoadedFilters().weight(), equalTo(1L));
        reader.close();
        writer.close();
        // There is no reference from readers and writer to any segment in the test index, so the size in the fbs cache must be 0
        assertThat(cache.getLoadedFilters().weight(), equalTo(0L));
    }",method,
"            @Override
            public void onCache(ShardId shardId, Accountable accountable) {
            }",method,
"            @Override
            public void onRemoval(ShardId shardId, Accountable accountable) {
            }",method,
"    public void testListener() throws IOException {
        IndexWriter writer = new IndexWriter(
                new RAMDirectory(),
                new IndexWriterConfig(new StandardAnalyzer()).setMergePolicy(new LogByteSizeMergePolicy())
        );
        Document document = new Document();
        document.add(new StringField(""field"", ""value"", Field.Store.NO));
        writer.addDocument(document);
        writer.commit();
        final DirectoryReader writerReader = DirectoryReader.open(writer);
        final IndexReader reader = ElasticsearchDirectoryReader.wrap(writerReader, new ShardId(""test"", ""_na_"", 0));
        final AtomicLong stats = new AtomicLong();
        final AtomicInteger onCacheCalls = new AtomicInteger();
        final AtomicInteger onRemoveCalls = new AtomicInteger();
        final BitsetFilterCache cache = new BitsetFilterCache(INDEX_SETTINGS, new BitsetFilterCache.Listener() {
            @Override
            public void onCache(ShardId shardId, Accountable accountable) {
                onCacheCalls.incrementAndGet();
                stats.addAndGet(accountable.ramBytesUsed());
                if (writerReader != reader) {
                    assertNotNull(shardId);
                    assertEquals(""test"", shardId.getIndexName());
                    assertEquals(0, shardId.id());
                } else {
                    assertNull(shardId);
                }
            }
            @Override
            public void onRemoval(ShardId shardId, Accountable accountable) {
                onRemoveCalls.incrementAndGet();
                stats.addAndGet(-accountable.ramBytesUsed());
                if (writerReader != reader) {
                    assertNotNull(shardId);
                    assertEquals(""test"", shardId.getIndexName());
                    assertEquals(0, shardId.id());
                } else {
                    assertNull(shardId);
                }
            }
        });
        BitSetProducer filter = cache.getBitSetProducer(new TermQuery(new Term(""field"", ""value"")));
        assertThat(matchCount(filter, reader), equalTo(1));
        assertTrue(stats.get() > 0);
        assertEquals(1, onCacheCalls.get());
        assertEquals(0, onRemoveCalls.get());
        IOUtils.close(reader, writer);
        assertEquals(1, onRemoveCalls.get());
        assertEquals(0, stats.get());
    }",method,
"            @Override
            public void onCache(ShardId shardId, Accountable accountable) {
                onCacheCalls.incrementAndGet();
                stats.addAndGet(accountable.ramBytesUsed());
                if (writerReader != reader) {
                    assertNotNull(shardId);
                    assertEquals(""test"", shardId.getIndexName());
                    assertEquals(0, shardId.id());
                } else {
                    assertNull(shardId);
                }
            }",method,
"                if (writerReader != reader) {
                    assertNotNull(shardId);
                    assertEquals(""test"", shardId.getIndexName());
                    assertEquals(0, shardId.id());
                }",method,
"            @Override
            public void onRemoval(ShardId shardId, Accountable accountable) {
                onRemoveCalls.incrementAndGet();
                stats.addAndGet(-accountable.ramBytesUsed());
                if (writerReader != reader) {
                    assertNotNull(shardId);
                    assertEquals(""test"", shardId.getIndexName());
                    assertEquals(0, shardId.id());
                } else {
                    assertNull(shardId);
                }
            }",method,
"                if (writerReader != reader) {
                    assertNotNull(shardId);
                    assertEquals(""test"", shardId.getIndexName());
                    assertEquals(0, shardId.id());
                }",method,
"    public void testSetNullListener() {
        try {
            new BitsetFilterCache(INDEX_SETTINGS, null);
            fail(""listener can't be null"");
        } catch (IllegalArgumentException ex) {
            assertEquals(""listener must not be null"", ex.getMessage());
            // all is well
        }
    }",method,
"    public void testRejectOtherIndex() throws IOException {
        BitsetFilterCache cache = new BitsetFilterCache(INDEX_SETTINGS, new BitsetFilterCache.Listener() {
            @Override
            public void onCache(ShardId shardId, Accountable accountable) {
            }
            @Override
            public void onRemoval(ShardId shardId, Accountable accountable) {
            }
        });
        Directory dir = newDirectory();
        IndexWriter writer = new IndexWriter(
                dir,
                newIndexWriterConfig()
        );
        writer.addDocument(new Document());
        DirectoryReader reader = DirectoryReader.open(writer);
        writer.close();
        reader = ElasticsearchDirectoryReader.wrap(reader, new ShardId(""test2"", ""_na_"", 0));
        BitSetProducer producer = cache.getBitSetProducer(new MatchAllDocsQuery());
        try {
            producer.getBitSet(reader.leaves().get(0));
            fail();
        } catch (IllegalStateException expected) {
            assertEquals(""Trying to load bit set for index [test2] with cache of index [test]"", expected.getMessage());
        } finally {
            IOUtils.close(reader, dir);
        }
    }",method,
"            @Override
            public void onCache(ShardId shardId, Accountable accountable) {
            }",method,
"            @Override
            public void onRemoval(ShardId shardId, Accountable accountable) {
            }",method,
"public class Match2ParseNode extends ParseNode {
    private final ParseNode receiverNode;
    private final ParseNode valueNode;
    public Match2ParseNode(SourceIndexLength position, ParseNode receiverNode, ParseNode valueNode) {
        super(position);
        assert receiverNode != null : ""receiverNode is not null"";
        assert valueNode != null : ""valueNode is not null"";
        this.receiverNode = receiverNode;
        this.valueNode = valueNode;
    }
    @Override
    public NodeType getNodeType() {
        return NodeType.MATCH2NODE;
    }
    @Override
    public <T> T accept(NodeVisitor<T> iVisitor) {
        return iVisitor.visitMatch2Node(this);
    }
    public ParseNode getReceiverNode() {
        return receiverNode;
    }
    public ParseNode getValueNode() {
        return valueNode;
    }
    @Override
    public List<ParseNode> childNodes() {
        return ParseNode.createList(receiverNode, valueNode);
    }
    @Override
    public boolean needsDefinitionCheck() {
        return false;
    }
}",class,
"    public Match2ParseNode(SourceIndexLength position, ParseNode receiverNode, ParseNode valueNode) {
        super(position);
        assert receiverNode != null : ""receiverNode is not null"";
        assert valueNode != null : ""valueNode is not null"";
        this.receiverNode = receiverNode;
        this.valueNode = valueNode;
    }",method,
"    @Override
    public NodeType getNodeType() {
        return NodeType.MATCH2NODE;
    }",method,
"    @Override
    public <T> T accept(NodeVisitor<T> iVisitor) {
        return iVisitor.visitMatch2Node(this);
    }",method,
"    public ParseNode getReceiverNode() {
        return receiverNode;
    }",method,
"    public ParseNode getValueNode() {
        return valueNode;
    }",method,
"    @Override
    public List<ParseNode> childNodes() {
        return ParseNode.createList(receiverNode, valueNode);
    }",method,
"    @Override
    public boolean needsDefinitionCheck() {
        return false;
    }",method,
"abstract class AbstractTreeWalker<N> {
  private enum State {STARTED, REQUESTED, PAUSED, FINISHED, FAILED}
  private final AtomicReference<State> state = new AtomicReference<>();
  private final AsyncPromise<TreePath> promise = new AsyncPromise<>();
  private final ArrayDeque<ArrayDeque<N>> stack = new ArrayDeque<>();
  private final Function<N, Object> converter;
  private final TreeVisitor visitor;
  private volatile TreePath current;
  public AbstractTreeWalker(@NotNull TreeVisitor visitor) {
    this(visitor, node -> node);
  }
  public AbstractTreeWalker(@NotNull TreeVisitor visitor, Function<N, Object> converter) {
    this.converter = converter;
    this.visitor = visitor;
  }
  protected abstract Collection<N> getChildren(@NotNull N node);
  public void setChildren(Collection<N> children) {
    boolean paused = state.compareAndSet(State.PAUSED, State.STARTED);
    if (!paused && !state.compareAndSet(State.REQUESTED, State.STARTED)) throw new IllegalStateException();
    stack.push(children == null ? new ArrayDeque<>() : new ArrayDeque<>(children));
    if (paused) processNextPath();
  }
  @NotNull
  public Promise<TreePath> promise() {
    return promise;
  }
  public void setError(@NotNull Throwable error) {
    state.set(State.FAILED);
    promise.setError(error);
  }
  public void start(N node) {
    start(null, node);
  }
  public void start(TreePath parent, N node) {
    TreePath result = null;
    if (node != null) {
      try {
        TreePath path = TreePathUtil.createTreePath(parent, converter.apply(node));
        switch (visitor.visit(path)) {
          case CONTINUE:
            update(null, State.REQUESTED);
            if (processChildren(path, node)) processNextPath();
            return;
          case INTERRUPT:
            result = path;
            break;
          case SKIP_CHILDREN:
            break;
          case SKIP_SIBLINGS:
            break;
        }
      }
      catch (Exception error) {
        setError(error);
      }
    }
    update(null, State.FINISHED);
    promise.setResult(result);
  }
  private boolean processChildren(@NotNull TreePath path, @NotNull N node) {
    current = path;
    Collection<N> children = getChildren(node);
    if (children == null) return !state.compareAndSet(State.REQUESTED, State.PAUSED);
    update(State.REQUESTED, State.STARTED);
    stack.push(new ArrayDeque<>(children));
    return true;
  }
  private void processNextPath() {
    try {
      while (State.STARTED == state.get()) {
        ArrayDeque<N> siblings = stack.peek();
        if (siblings == null) {
          update(State.STARTED, State.FINISHED);
          current = null;
          promise.setResult(null);
          return; // nothing to process
        }
        N node = siblings.poll();
        if (node == null) {
          TreePath path = current;
          if (path == null) throw new IllegalStateException();
          if (siblings != stack.poll()) throw new IllegalStateException();
          current = path.getParentPath();
        }
        else {
          TreePath path = TreePathUtil.createTreePath(current, converter.apply(node));
          switch (visitor.visit(path)) {
            case CONTINUE:
              update(State.STARTED, State.REQUESTED);
              if (processChildren(path, node)) break;
              return; // stop processing and wait for setChildren
            case INTERRUPT:
              update(State.STARTED, State.FINISHED);
              current = null;
              stack.clear();
              promise.setResult(path);
              return; // path is found
            case SKIP_SIBLINGS:
              siblings.clear();
              break;
            case SKIP_CHILDREN:
              break;
          }
        }
      }
    }
    catch (Exception error) {
      setError(error);
    }
  }
  private void update(State expected, State replacement) {
    if (!state.compareAndSet(expected, replacement)) throw new IllegalStateException();
  }
}",class,
"  public AbstractTreeWalker(@NotNull TreeVisitor visitor) {
    this(visitor, node -> node);
  }",method,
"  public AbstractTreeWalker(@NotNull TreeVisitor visitor, Function<N, Object> converter) {
    this.converter = converter;
    this.visitor = visitor;
  }",method,
"  public void setChildren(Collection<N> children) {
    boolean paused = state.compareAndSet(State.PAUSED, State.STARTED);
    if (!paused && !state.compareAndSet(State.REQUESTED, State.STARTED)) throw new IllegalStateException();
    stack.push(children == null ? new ArrayDeque<>() : new ArrayDeque<>(children));
    if (paused) processNextPath();
  }",method,
"  @NotNull
  public Promise<TreePath> promise() {
    return promise;
  }",method,
"  public void setError(@NotNull Throwable error) {
    state.set(State.FAILED);
    promise.setError(error);
  }",method,
"  public void start(N node) {
    start(null, node);
  }",method,
"  public void start(TreePath parent, N node) {
    TreePath result = null;
    if (node != null) {
      try {
        TreePath path = TreePathUtil.createTreePath(parent, converter.apply(node));
        switch (visitor.visit(path)) {
          case CONTINUE:
            update(null, State.REQUESTED);
            if (processChildren(path, node)) processNextPath();
            return;
          case INTERRUPT:
            result = path;
            break;
          case SKIP_CHILDREN:
            break;
          case SKIP_SIBLINGS:
            break;
        }
      }
      catch (Exception error) {
        setError(error);
      }
    }
    update(null, State.FINISHED);
    promise.setResult(result);
  }",method,
"    if (node != null) {
      try {
        TreePath path = TreePathUtil.createTreePath(parent, converter.apply(node));
        switch (visitor.visit(path)) {
          case CONTINUE:
            update(null, State.REQUESTED);
            if (processChildren(path, node)) processNextPath();
            return;
          case INTERRUPT:
            result = path;
            break;
          case SKIP_CHILDREN:
            break;
          case SKIP_SIBLINGS:
            break;
        }
      }
      catch (Exception error) {
        setError(error);
      }
    }",method,
"      catch (Exception error) {
        setError(error);
      }",method,
"  private boolean processChildren(@NotNull TreePath path, @NotNull N node) {
    current = path;
    Collection<N> children = getChildren(node);
    if (children == null) return !state.compareAndSet(State.REQUESTED, State.PAUSED);
    update(State.REQUESTED, State.STARTED);
    stack.push(new ArrayDeque<>(children));
    return true;
  }",method,
"  private void processNextPath() {
    try {
      while (State.STARTED == state.get()) {
        ArrayDeque<N> siblings = stack.peek();
        if (siblings == null) {
          update(State.STARTED, State.FINISHED);
          current = null;
          promise.setResult(null);
          return; // nothing to process
        }
        N node = siblings.poll();
        if (node == null) {
          TreePath path = current;
          if (path == null) throw new IllegalStateException();
          if (siblings != stack.poll()) throw new IllegalStateException();
          current = path.getParentPath();
        }
        else {
          TreePath path = TreePathUtil.createTreePath(current, converter.apply(node));
          switch (visitor.visit(path)) {
            case CONTINUE:
              update(State.STARTED, State.REQUESTED);
              if (processChildren(path, node)) break;
              return; // stop processing and wait for setChildren
            case INTERRUPT:
              update(State.STARTED, State.FINISHED);
              current = null;
              stack.clear();
              promise.setResult(path);
              return; // path is found
            case SKIP_SIBLINGS:
              siblings.clear();
              break;
            case SKIP_CHILDREN:
              break;
          }
        }
      }
    }
    catch (Exception error) {
      setError(error);
    }
  }",method,
"        if (siblings == null) {
          update(State.STARTED, State.FINISHED);
          current = null;
          promise.setResult(null);
          return; // nothing to process
        }",method,
"        if (node == null) {
          TreePath path = current;
          if (path == null) throw new IllegalStateException();
          if (siblings != stack.poll()) throw new IllegalStateException();
          current = path.getParentPath();
        }",method,
"    catch (Exception error) {
      setError(error);
    }",method,
"  private void update(State expected, State replacement) {
    if (!state.compareAndSet(expected, replacement)) throw new IllegalStateException();
  }",method,
"public final class NetworkInfo {
  public static class Event extends AbstractBuckEvent {
    Network network;
    public Event(Network network) {
      super(EventKey.unique());
      this.network = network;
    }
    public Network getNetwork() {
      return network;
    }
    @Override
    public String getEventName() {
      return ""NetworkInfoEvent"";
    }
    @Override
    protected String getValueString() {
      return network.toString();
    }
  }
  // Buck's own integration tests will run with this system property
  // set to false.
  //
  // Otherwise, we would need to add libjcocoa.dylib to
  // java.library.path, which could interfere with external Java
  // tests' own C library dependencies.
  private static final boolean ENABLE_OBJC = Boolean.getBoolean(""buck.enable_objc"");
  private NetworkInfo() {}
  public static void generateActiveNetworkAsync(
      ExecutorService executorService, BuckEventBus buckEventBus) {
    executorService.submit(
        () -> {
          buckEventBus.post(new Event(getLikelyActiveNetwork()));
        });
  }
  public static Network getLikelyActiveNetwork() {
    if (ENABLE_OBJC) {
      return MacNetworkConfiguration.getLikelyActiveNetwork();
    }
    return new Network(NetworkMedium.UNKNOWN);
  }
  public static Optional<String> getWifiSsid() {
    // TODO(royw): Support Linux and Windows.
    if (ENABLE_OBJC) {
      return MacWifiSsidFinder.findCurrentSsid();
    }
    return Optional.empty();
  }
}",class,
"  public static class Event extends AbstractBuckEvent {
    Network network;
    public Event(Network network) {
      super(EventKey.unique());
      this.network = network;
    }
    public Network getNetwork() {
      return network;
    }
    @Override
    public String getEventName() {
      return ""NetworkInfoEvent"";
    }
    @Override
    protected String getValueString() {
      return network.toString();
    }
  }",class,
"    public Event(Network network) {
      super(EventKey.unique());
      this.network = network;
    }",method,
"    public Network getNetwork() {
      return network;
    }",method,
"    @Override
    public String getEventName() {
      return ""NetworkInfoEvent"";
    }",method,
"    @Override
    protected String getValueString() {
      return network.toString();
    }",method,
  private NetworkInfo() {},method,
"  public static void generateActiveNetworkAsync(
      ExecutorService executorService, BuckEventBus buckEventBus) {
    executorService.submit(
        () -> {
          buckEventBus.post(new Event(getLikelyActiveNetwork()));
        });
  }",method,
"  public static Network getLikelyActiveNetwork() {
    if (ENABLE_OBJC) {
      return MacNetworkConfiguration.getLikelyActiveNetwork();
    }
    return new Network(NetworkMedium.UNKNOWN);
  }",method,
"    if (ENABLE_OBJC) {
      return MacNetworkConfiguration.getLikelyActiveNetwork();
    }",method,
"  public static Optional<String> getWifiSsid() {
    // TODO(royw): Support Linux and Windows.
    if (ENABLE_OBJC) {
      return MacWifiSsidFinder.findCurrentSsid();
    }
    return Optional.empty();
  }",method,
"    if (ENABLE_OBJC) {
      return MacWifiSsidFinder.findCurrentSsid();
    }",method,
"  Platform(String platformName, String autoconfName) {
    this.platformName = platformName;
    this.autoconfName = autoconfName;
  }",method,
"  public String getAutoconfName() {
    return autoconfName;
  }",method,
"  public String getPrintableName() {
    return platformName;
  }",method,
"  public static Platform detect() {
    String platformName = System.getProperty(""os.name"");
    if (platformName == null) {
      return UNKNOWN;
    } else if (platformName.startsWith(""Linux"")) {
      return LINUX;
    } else if (platformName.startsWith(""Mac OS"")) {
      return MACOS;
    } else if (platformName.startsWith(""Windows"")) {
      return WINDOWS;
    } else if (platformName.startsWith(""FreeBSD"")) {
      return FREEBSD;
    } else {
      return UNKNOWN;
    }
  }",method,
"    if (platformName == null) {
      return UNKNOWN;
    }",method,
"public class AuthCheckFilter implements Filter {
    AccountMgr accountMgr;
    public AccountMgr getAccountMgr() {
        return accountMgr;
    }
    public void setAccountMgr(AccountMgr accountMgr) {
        this.accountMgr = accountMgr;
    }
    public void destroy() {}
    public void doFilter(ServletRequest request, ServletResponse response,
                         FilterChain chain) throws IOException, ServletException {
        String url = null;
        if (request instanceof HttpServletRequest) {
            url = ((HttpServletRequest) request).getRequestURL().toString();
        }
        String domain = URLUtils.getDomain(url);
        if (domain != """") {
            SystemConstant.setDOMAIN_URL(domain);
        }
        // all requests count into realtime charts
        SystemVisitorLog.count();
        if (URLUtils.shouldLog(url))
            SystemVisitorLog.count(request.getRemoteAddr());
        if (SystemConstant.DOMAIN_URL.isEmpty()) {
            SystemConstant.DOMAIN_URL = request.getServerName();
            if (request.getServerPort() != 80) {
                SystemConstant.DOMAIN_URL += "":"" + request.getServerPort();
            }
        }
        HttpSession session = ((HttpServletRequest) request).getSession();
        Object userAccount = session.getAttribute(ContextManager.KEY_ACCOUNT);
        Object userName = session.getAttribute(ContextManager.KEY_NAME);
        boolean logined = userAccount != null;
        SystemConstant.README_PATH = session.getServletContext().getRealPath(File.separator + ""README.md"");
        SystemConstant.ROOT = session.getServletContext().getRealPath(File.separator);
        if (!logined) {
            SimpleSSOUser user = SimpleUserUtil
                    .findUser((HttpServletRequest) request);
            if (user != null) {
                SystemConstant.user = user;
                String emailPrefix = user.getEmailPrefix();
                User rapUser = accountMgr.getUser(emailPrefix);
                if (rapUser == null) {
                    // proceed register
                    User newUser = new User();
                    newUser.setAccount(emailPrefix);
                    newUser.setPassword(""RESERVED"");
                    String name = user.getNickNameCn();
                    if (name == null || name.isEmpty()) {
                        name = user.getLastName();
                    }
                    newUser.setName(name);
                    newUser.setEmail(user.getEmailAddr());
                    newUser.setRealname(user.getLastName());
                    newUser.setEmpId(user.getEmpId());
                    getAccountMgr().addUser(newUser);
                    rapUser = accountMgr.getUser(emailPrefix);
                    if (rapUser == null) {
                        try {
                            throw new Exception(""user register failed!"");
                        } catch (Exception e) {
                            e.printStackTrace();
                        }
                    }
                }
                // proceed login
                session.setAttribute(ContextManager.KEY_ACCOUNT, rapUser.getAccount());
                session.setAttribute(ContextManager.KEY_USER_ID, rapUser.getId());
                session.setAttribute(ContextManager.KEY_NAME, rapUser.getName());
                session.setAttribute(ContextManager.KEY_EMP_ID, rapUser.getEmpId());
                Set<Role> roleList = new HashSet<Role>();
                for (Role role : rapUser.getRoleList()) {
                    Role copied = new Role();
                    copied.setId(role.getId());
                    copied.setName(role.getName());
                    roleList.add(copied);
                }
                session.setAttribute(ContextManager.KEY_ROLE_LIST, roleList);
            }
        } else {
            if (URLUtils.shouldLog(url)) {
                User logUser = new User();
                logUser.setAccount((String) userAccount);
                logUser.setName((String) userName);
                SystemVisitorLog.count(logUser);
            }
        }
        chain.doFilter(request, response);
    }
    public void init(FilterConfig arg0) throws ServletException {
    }
}",class,
"    public AccountMgr getAccountMgr() {
        return accountMgr;
    }",method,
